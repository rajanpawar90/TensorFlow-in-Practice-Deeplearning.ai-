{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Course2_Summary.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajanpawar90/TensorFlow-in-Practice-Deeplearning.ai-/blob/master/SummaryCode/Course2_Summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQbQc9VmIPMJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " #Install the latest version of tensorflow\n",
        " !pip install --upgrade tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGO9c49XzpRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#rock-paper-scissors data extracrt images from base directory zip files\n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip \\\n",
        "    -O /tmp/rps.zip\n",
        "  \n",
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip \\\n",
        "    -O /tmp/rps-test-set.zip\n",
        "\n",
        "\n",
        " \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYHl3n5qLA4q",
        "colab_type": "text"
      },
      "source": [
        "#Data Preprocessing\n",
        "Let's set up data generators that will read pictures in our source folders, convert them to float32 tensors, and feed them (with their labels) to our network. We'll have one generator for the training images and one for the validation images. Our generators will yield batches of 20 images of size 150x150 and their labels (binary).\n",
        "\n",
        "As you may already know, data that goes into neural networks should usually be normalized in some way to make it more amenable to processing by the network. (It is uncommon to feed raw pixels into a convnet.) In our case, we will preprocess our images by normalizing the pixel values to be in the [0, 1] range (originally all values are in the [0, 255] range).\n",
        "\n",
        "In Keras this can be done via the keras.preprocessing.image.ImageDataGenerator class using the rescale parameter. This ImageDataGenerator class allows you to instantiate generators of augmented image batches (and their labels) via .flow(data, labels) or .flow_from_directory(directory). These generators can then be used with the Keras model methods that accept data generators as inputs: fit_generator, evaluate_generator, and predict_generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMap_H5TBQB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################################################################################################\n",
        "#Function for generating ImageDataGenerator object for all datasets in this program. This datatype is necessay for fitting the model \n",
        "#####################################################################################################################################\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras_preprocessing\n",
        "from keras_preprocessing import image\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "def getTrainValidDatagens(train_dir , valid_dir):\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale = 1./255,\n",
        "      rotation_range = 40,\n",
        "      width_shift_range = 0.2,\n",
        "      height_shift_range = 0.2,\n",
        "      shear_range = 0.2,\n",
        "      zoom_range = 0.2,\n",
        "      horizontal_flip = True,\n",
        "      fill_mode = 'nearest')\n",
        "  \n",
        "  train_images = train_datagen.flow_from_directory(\n",
        "      train_dir,\n",
        "      target_size = (150, 150)\n",
        "      #class_mode = 'categorical'\n",
        "  )\n",
        "\n",
        "  valid_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "  valid_images = valid_datagen.flow_from_directory(\n",
        "      valid_dir,\n",
        "      target_size = (150, 150)\n",
        "      #class_mode = 'categorical'\n",
        "  )\n",
        "\n",
        "  return train_images , valid_images"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8vo_fN8LKIm",
        "colab_type": "text"
      },
      "source": [
        "#Create train and test datasets from downloaded zip file\n",
        "The contents of the .zip are extracted to the base directory /tmp/, which contains train (/tmp/rps) and validation subdirectories (/tmp/rps-test-set) for the training and validation datasets (see the Machine Learning Crash Course for a refresher on training, validation, and test sets), which in turn each contain rock, paper and scissor subdirectories.\n",
        "\n",
        "In short: The training set is the data that is used to tell the neural network model that 'this is what a rock looks like', 'this is what a paper looks like' etc. The validation data set is images of rock, paper and scissors that the neural network will not see as part of the training, so you can test how well or how badly it does in evaluating if an image contains a cat or a dog.\n",
        "\n",
        "One thing to pay attention to in this sample: We do not explicitly label the images of rocks, paper or scissors. If you remember with the handwriting example earlier, we had labelled 'this is a 1', 'this is a 7' etc. Later you'll see something called an ImageGenerator being used -- and this is coded to read images from subdirectories, and automatically label them from the name of that subdirectory. So, for example, you will have a 'training' directory containing a 'rock', 'paper' and 'scissors' directory. ImageGenerator will label the images appropriately for you, reducing a coding step.\n",
        "\n",
        "Let's define each of these directories:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtLq7W1u__YX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################################################################\n",
        "#Download signs dataset and extract train, test data from the appropraite directories              \n",
        "####################################################################################################\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/rps.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "local_zip = '/tmp/rps-test-set.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "\n",
        "rock_dir = os.path.join('/tmp/rps/rock')\n",
        "paper_dir = os.path.join('/tmp/rps/paper')\n",
        "scissors_dir = os.path.join('/tmp/rps/scissors')\n",
        "\n",
        "#Peak in the dataset\n",
        "print('total training rock images:', len(os.listdir(rock_dir)))\n",
        "print('total training paper images:', len(os.listdir(paper_dir)))\n",
        "print('total training scissors images:', len(os.listdir(scissors_dir)))\n",
        "\n",
        "rock_files = os.listdir(rock_dir)\n",
        "print(rock_files[:10])\n",
        "\n",
        "paper_files = os.listdir(paper_dir)\n",
        "print(paper_files[:10])\n",
        "\n",
        "scissors_files = os.listdir(scissors_dir)\n",
        "print(scissors_files[:10])\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "pic_index = 2\n",
        "\n",
        "next_rock = [os.path.join(rock_dir, fname) \n",
        "                for fname in rock_files[pic_index-2:pic_index]]\n",
        "next_paper = [os.path.join(paper_dir, fname) \n",
        "                for fname in paper_files[pic_index-2:pic_index]]\n",
        "next_scissors = [os.path.join(scissors_dir, fname) \n",
        "                for fname in scissors_files[pic_index-2:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_rock+next_paper+next_scissors):\n",
        "  #print(img_path)\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "  plt.axis('Off')\n",
        "  plt.show()\n",
        "\n",
        "#Train and validation generator for fitting the model\n",
        "#train data is stored in '/tpm/rps' and validation data in '/tms/rps-test-set'\n",
        "rps_train_generator_images , rps_valid_generator_images = getTrainValidDatagens(train_dir = '/tmp/rps', valid_dir = '/tmp/rps-test-set')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QJM3mHMzpes",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################################################################\n",
        "#Download Cats-v-Dogs dataset from Kaggle and create tran and test datasets from the raw data file \n",
        "####################################################################################################\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "\n",
        "# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL to the dataset\n",
        "\n",
        "# Note: This is a very large dataset and will take time to download\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n",
        "    -O \"/tmp/cats-and-dogs.zip\"\n",
        "\n",
        "local_zip = '/tmp/cats-and-dogs.zip'\n",
        "zip_ref   = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()\n",
        "print(len(os.listdir('/tmp/PetImages/Cat/')))\n",
        "print(len(os.listdir('/tmp/PetImages/Dog/')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZpxrKB_Pp2z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "try:\n",
        "    os.mkdir('/tmp/cats-v-dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n",
        "    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "    files = []\n",
        "    for filename in os.listdir(SOURCE):\n",
        "        file = SOURCE + filename\n",
        "        if os.path.getsize(file) > 0:\n",
        "            files.append(filename)\n",
        "        else:\n",
        "            print(filename + \" is zero length, so ignoring.\")\n",
        "\n",
        "    training_length = int(len(files) * SPLIT_SIZE)\n",
        "    testing_length = int(len(files) - training_length)\n",
        "    shuffled_set = random.sample(files, len(files))\n",
        "    training_set = shuffled_set[0:training_length]\n",
        "    testing_set = shuffled_set[:testing_length]\n",
        "\n",
        "    for filename in training_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TRAINING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "    for filename in testing_set:\n",
        "        this_file = SOURCE + filename\n",
        "        destination = TESTING + filename\n",
        "        copyfile(this_file, destination)\n",
        "\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n",
        "TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n",
        "\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n",
        "TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n",
        "\n",
        "split_size = .9\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "TRAIN_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "VALID_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "#Train and validation generator for fitting the model\n",
        "#train data is stored in '/tmp/cats-v-dog/training' and validation data in '/tmp/cats-d-dogs/testing'\n",
        "catsDogs_train_generator_images , catsDogs_valid_generator_images = getTrainValidDatagens(train_dir = TRAIN_DIR, valid_dir = VALID_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMn9hs4aYOhT",
        "colab_type": "text"
      },
      "source": [
        "The data for this exercise is available at: https://www.kaggle.com/datamunge/sign-language-mnist/home\n",
        "\n",
        "Sign up and download to find 2 CSV files: sign_mnist_test.csv and sign_mnist_train.csv -- You will upload both of them using this button before you can continue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzjmwZxaYZHy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##################################################################################\n",
        "# Upload the raw datafile from computer's HD and generate train and test datasets \n",
        "##################################################################################\n",
        "from google.colab import files\n",
        "\n",
        "upoaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AhNGWJpEV7QS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(filename):\n",
        "  with open(filename) as training_file:\n",
        "    csv_reader = csv.reader(training_file, delimiter= ',')\n",
        "    first_line = True\n",
        "    temp_images = []\n",
        "    temp_labels = []\n",
        "    for row in csv_reader:\n",
        "      if first_line:\n",
        "        # print(\"Ignoring first line\")\n",
        "        first_line = False\n",
        "      else:\n",
        "        temp_labels.append(row[0])\n",
        "        image_data = row[1:785]\n",
        "        image_data_as_array = np.array_split(image_data, 28)\n",
        "        temp_images.append(image_data_as_array)\n",
        "    images = np.array(temp_images).astype('float')\n",
        "    labels = np.array(temp_labels).astype('float')\n",
        "  return images, labels\n",
        "\n",
        "training_images, training_labels = get_data('sign_mnist_train.csv')\n",
        "testing_images, testing_labels = get_data('sign_mnist_test.csv')\n",
        "\n",
        "print(training_images.shape)\n",
        "print(training_labels.shape)\n",
        "print(testing_images.shape)\n",
        "print(testing_labels.shape)\n",
        "\n",
        "training_images = np.exapnd_dims(training_images, axis=3)\n",
        "testing_images = np/expand_dims(testing_images, axis=3)\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255,\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest')\n",
        "\n",
        "validation_datagen = ImageDataGenerator(\n",
        "    rescale=1. / 255)\n",
        "\n",
        "##Train and test image datasets to be fed in the model\n",
        "uploaded_train_generated_images = train_datagen.flow(training_images, training_labels, batch_size=32)\n",
        "uploaded_test_generated_images = validation_datagen.flow(testing_images, testing_labels, batch_size=32)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tM-zHjljD89H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################\n",
        "# Functions for call back and plotting model results \n",
        "######################################################\n",
        "\n",
        "import matplotlib.image  as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plotCharts(history):\n",
        "  acc = history.history['accuracy']\n",
        "  loss = history.history['loss']\n",
        "  val_acc = history.history['val_accuracy']\n",
        "  val_loss = history.history['val_loss']\n",
        "  epochs = range(len(acc))\n",
        "\n",
        "  plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "  plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "  plt.title('Training and validation accuracy')\n",
        "  plt.figure()\n",
        "\n",
        "  plt.plot(epochs, loss, 'r', label='Training loss')\n",
        "  plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "  plt.title('Training and validation loss')\n",
        "  plt.figure()\n",
        "\n",
        "# Define a Callback class that stops training once accuracy reaches 99.9%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy')>0.999):\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1OaF5bg0sA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "###################################################\n",
        "#Binary, Categorical and transfer learning models \n",
        "###################################################\n",
        "\n",
        "def runThreeClassModel(train_generator_images , valid_generator_images, epochs):\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "      # This is the first convolution\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(2, 2),\n",
        "      # The second convolution\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # The third convolution\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # The fourth convolution\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      # 512 neuron hidden layer\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(3, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  callbacks = myCallback()\n",
        "  history = model.fit(train_generator_images, epochs = epochs, validation_data = valid_generator_images, verbose = 1, callbacks=[callbacks])\n",
        "  plotCharts(history)\n",
        "  #visualize(model)\n",
        "\n",
        "def runBinaryClassModel(train_generator_images , valid_generator_images, epochs):\n",
        "  model = tf.keras.models.Sequential([\n",
        "      # Note the input shape is the desired size of the image 150x150 with 3 bytes color\n",
        "      # This is the first convolution\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(150, 150, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(2, 2),\n",
        "      # The second convolution\n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # The third convolution\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # The fourth convolution\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dropout(0.5),\n",
        "      # 512 neuron hidden layer\n",
        "      tf.keras.layers.Dense(512, activation='relu'),\n",
        "      tf.keras.layers.Dense(2, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  model.summary()\n",
        "  model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
        "  callbacks = myCallback()\n",
        "  history = model.fit(train_generator_images, epochs = epochs, validation_data = valid_generator_images, verbose = 1, callbacks=[callbacks])\n",
        "  plotCharts(history)\n",
        "  #visualize(model)\n",
        "\n",
        "#Model for transfer learning using InceptionV3\n",
        "def runTransferInception(train_generator_images , valid_generator_images, epochs):\n",
        "  # Download the inception v3 weights\n",
        "  !wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n",
        "    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
        "\n",
        "  # Import the inception model  \n",
        "  from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "  from tensorflow.keras import Model\n",
        "  # Create an instance of the inception model from the local pre-trained weights\n",
        "  local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n",
        "\n",
        "  pre_trained_model = InceptionV3(input_shape=(150, 150, 3),\n",
        "                                 include_top = False,\n",
        "                                 weights = None)\n",
        "  \n",
        "  pre_trained_model.load_weights(local_weights_file)\n",
        "\n",
        "  #Make all layers in pre-trained model non-trainable\n",
        "  from tensorflow.keras import layers\n",
        "  for layer in pre_trained_model.layers:\n",
        "    layer.trainable = False\n",
        "  \n",
        "  #pre_trained_model.summary()\n",
        "\n",
        "  last_layer = pre_trained_model.get_layer('mixed7')\n",
        "  print('last layer output shape: ', last_layer.output_shape)\n",
        "  last_output = last_layer.output\n",
        "  \n",
        "  from tensorflow.keras.optimizers import RMSprop\n",
        " # Flatten the output layer to 1 dimension\n",
        "  x = layers.Flatten()(last_output)\n",
        "  # Add a fully connected layer with 1,024 hidden units and ReLU activation\n",
        "  x = layers.Dense(1024, activation='relu')(x)\n",
        "  # Add a dropout rate of 0.2\n",
        "  x = layers.Dropout(0.2)(x)                  \n",
        "  # Add a final sigmoid layer for classification\n",
        "  x = layers.Dense  (3, activation='sigmoid')(x)           \n",
        "\n",
        "  model = Model( pre_trained_model.input, x) \n",
        "\n",
        "  model.compile(optimizer = RMSprop(lr=0.0001), \n",
        "                loss = 'categorical_crossentropy', \n",
        "                metrics = ['accuracy'])\n",
        "\n",
        "  model.summary()\n",
        "\n",
        "  callbacks=myCallback()\n",
        "  history = model.fit_generator(\n",
        "      train_generator_images,\n",
        "      validation_data = valid_generator_images,\n",
        "      steps_per_epoch = 100,\n",
        "      epochs=epochs,\n",
        "      validation_steps= 50,\n",
        "      verbose = 2,\n",
        "      callbacks=[callbacks],\n",
        "  )\n",
        "  plotCharts(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A0pNgvSb05ea",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Run models on rps data\n",
        "\n",
        "runThreeClassModel(rps_train_generator_images, rps_valid_generator_images, epochs=1)\n",
        "runTransferInception(rps_train_generator_images, rps_valid_generator_images, epochs=1)\n",
        "\n",
        "#Run models on cats and dogs data \n",
        "\n",
        "runBinaryClassModel(catsDogs_train_generator_images, catsDogs_valid_generator_images, epochs=1)\n",
        "runTransferInception(catsDogs_train_generator_images, catsDogs_valid_generator_images, epochs=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFxr8mGVNCkV",
        "colab_type": "text"
      },
      "source": [
        "#Visualizing Intermediate Representations\n",
        "To get a feel for what kind of features our convnet has learned, one fun thing to do is to visualize how an input gets transformed as it goes through the convnet.\n",
        "\n",
        "Let's pick a random cat or dog image from the training set, and then generate a figure where each row is the output of a layer, and each image in the row is a specific filter in that output feature map. Rerun this cell to generate intermediate representations for a variety of training images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6j6kqIb07cb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Function for visualizing intermediate images of convolutions\n",
        "\n",
        "def visualize(model):\n",
        "  #visualize responses\n",
        "  import numpy as np\n",
        "  import random\n",
        "  from   tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
        "\n",
        "  # Let's define a new Model that will take an image as input, and will output\n",
        "  # intermediate representations for all layers in the previous model after\n",
        "  # the first.\n",
        "  successive_outputs = [layer.output for layer in model.layers[1:]]\n",
        "\n",
        "  #visualization_model = Model(img_input, successive_outputs)\n",
        "  visualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n",
        "\n",
        "  # Let's prepare a random input image of a cat or dog from the training set.\n",
        "  cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
        "  dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
        "\n",
        "  img_path = random.choice(cat_img_files + dog_img_files)\n",
        "  img = load_img(img_path, target_size=(150, 150))  # this is a PIL image\n",
        "\n",
        "  x   = img_to_array(img)                           # Numpy array with shape (150, 150, 3)\n",
        "  x   = x.reshape((1,) + x.shape)                   # Numpy array with shape (1, 150, 150, 3)\n",
        "\n",
        "  # Rescale by 1/255\n",
        "  x /= 255.0\n",
        "\n",
        "  # Let's run our image through our network, thus obtaining all\n",
        "  # intermediate representations for this image.\n",
        "  successive_feature_maps = visualization_model.predict(x)\n",
        "\n",
        "  # These are the names of the layers, so can have them as part of our plot\n",
        "  layer_names = [layer.name for layer in model.layers]\n",
        "\n",
        "  # -----------------------------------------------------------------------\n",
        "  # Now let's display our representations\n",
        "  # -----------------------------------------------------------------------\n",
        "  for layer_name, feature_map in zip(layer_names, successive_feature_maps):\n",
        "    \n",
        "    if len(feature_map.shape) == 4:\n",
        "      \n",
        "      #-------------------------------------------\n",
        "      # Just do this for the conv / maxpool layers, not the fully-connected layers\n",
        "      #-------------------------------------------\n",
        "      n_features = feature_map.shape[-1]  # number of features in the feature map\n",
        "      size       = feature_map.shape[ 1]  # feature map shape (1, size, size, n_features)\n",
        "      \n",
        "      # We will tile our images in this matrix\n",
        "      display_grid = np.zeros((size, size * n_features))\n",
        "      \n",
        "      #-------------------------------------------------\n",
        "      # Postprocess the feature to be visually palatable\n",
        "      #-------------------------------------------------\n",
        "      for i in range(n_features):\n",
        "        x  = feature_map[0, :, :, i]\n",
        "        x -= x.mean()\n",
        "        x /= x.std ()\n",
        "        x *=  64\n",
        "        x += 128\n",
        "        x  = np.clip(x, 0, 255).astype('uint8')\n",
        "        display_grid[:, i * size : (i + 1) * size] = x # Tile each filter into a horizontal grid\n",
        "\n",
        "      #-----------------\n",
        "      # Display the grid\n",
        "      #-----------------\n",
        "\n",
        "      scale = 20. / n_features\n",
        "      plt.figure( figsize=(scale * n_features, scale) )\n",
        "      plt.title ( layer_name )\n",
        "      plt.grid  ( False )\n",
        "      plt.imshow( display_grid, aspect='auto', cmap='viridis' ) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8n6eN9U7NNCL",
        "colab_type": "text"
      },
      "source": [
        "As you can see we go from the raw pixels of the images to increasingly abstract and compact representations. The representations downstream start highlighting what the network pays attention to, and they show fewer and fewer features being \"activated\"; most are set to zero. This is called \"sparsity.\" Representation sparsity is a key feature of deep learning.\n",
        "\n",
        "These representations carry increasingly less information about the original pixels of the image, but increasingly refined information about the class of the image. You can think of a convnet (or a deep network in general) as an information distillation pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVBkCHGY5Mil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Upload rock/paper/scissor images and run models on them to predict it\n",
        "\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(150, 150))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TN9Q1Q4Rp-dN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}